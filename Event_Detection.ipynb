{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We obtain the events, transform to the date format and filter for the year 2019\n",
    "data_raw = pd.read_csv('Events_BBDD.csv')\n",
    "data_raw['AIS_UPDATE_DATE'] = pd.to_datetime(data_raw['AIS_UPDATE_DATE'], format = '%Y-%m-%d %H:%M:%S')\n",
    "data_raw = data_raw[data_raw['AIS_UPDATE_DATE'].dt.year == 2019]\n",
    "data = data_raw.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We filter for only events of interest 6: Pilotage Start, 7: Towing Start, 27: Excessive Speed\n",
    "data = data[data['ID_EVENT_TYPE'].isin([6,7,27])]\n",
    "data = data[(data['AIS_LAT'] < 90) & (data['AIS_LAT'] > -90) ]\n",
    "data = data[(data['AIS_LON'] < 180) & (data['AIS_LON'] > -180)]\n",
    "data = data.dropna(subset = ['AIS_LAT', 'AIS_LON'])\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gmaps\n",
    "import gmaps.datasets\n",
    "from ipywidgets.embed import embed_minimal_html\n",
    "\n",
    "#For each of the events, we make heat maps to see the areas where they are usually detected\n",
    "gmaps.configure(api_key='')\n",
    "temp_6 = pd.DataFrame()\n",
    "temp_6['latitude'] = data[data['ID_EVENT_TYPE'] == 6]['AIS_LAT']\n",
    "temp_6['longitude'] = data[data['ID_EVENT_TYPE'] == 6]['AIS_LON']\n",
    "fig = gmaps.figure(map_type='TERRAIN', display_toolbar = True)\n",
    "heatmap_layer = gmaps.heatmap_layer(temp_6)\n",
    "fig.add_layer(heatmap_layer)\n",
    "embed_minimal_html('PilotageStartsArea.html', views=[fig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaps.configure(api_key='')\n",
    "temp_7 = pd.DataFrame()\n",
    "temp_7  ['latitude'] = data[data['ID_EVENT_TYPE'] == 7]['AIS_LAT']\n",
    "temp_7['longitude'] = data[data['ID_EVENT_TYPE'] == 7]['AIS_LON']\n",
    "fig = gmaps.figure(map_type='ROADMAP', display_toolbar = True)\n",
    "heatmap_layer = gmaps.heatmap_layer(temp_7)\n",
    "fig.add_layer(heatmap_layer)\n",
    "embed_minimal_html('TowingStarsArea.html', views=[fig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaps.configure(api_key='')\n",
    "temp_27 = pd.DataFrame()\n",
    "temp_27['latitude'] = data[data['ID_EVENT_TYPE'] == 27]['AIS_LAT']\n",
    "temp_27['longitude'] = data[data['ID_EVENT_TYPE'] == 27]['AIS_LON']\n",
    "fig = gmaps.figure(map_type='ROADMAP', display_toolbar = True)\n",
    "heatmap_layer = gmaps.heatmap_layer(temp_27)\n",
    "fig.add_layer(heatmap_layer)\n",
    "embed_minimal_html('SpeedExceedAreas.html', views=[fig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "#We represent the events detected for each of the months in a bar plot.\n",
    "df_plot = pd.DataFrame()\n",
    "df_plot['event'] = data_raw['ID_EVENT_TYPE']\n",
    "df_plot['integer'] = 1\n",
    "df_plot['event'] = data_raw['ID_EVENT_TYPE']\n",
    "df_plot['time'] = data_raw['AIS_UPDATE_DATE']\n",
    "dict_events = {'others' : 'Others Events', 6 : 'Pilotage Start', 7: 'Towage Start', 27: 'Speed Exceed'}\n",
    "months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "month_index = dict(zip(months, range(len(months))))\n",
    "\n",
    "df_plot['event_desc'] = df_plot['event'].map(dict_events)\n",
    "df_plot['month']= df_plot['time'].apply(lambda x: x.month_name())\n",
    "df_plot = df_plot.groupby(['event_desc', 'month'])['integer'].sum()\n",
    "df_plot = df_plot.reset_index()\n",
    "df_plot['month_rank'] = df_plot['month'].map(month_index)\n",
    "df_plot = df_plot.sort_values(by =['month_rank'])\n",
    "\n",
    "g = sns.catplot(data = df_plot, x='month', y = 'integer', hue = 'event_desc', kind = 'bar')\n",
    "g.set_xticklabels(rotation = 45)\n",
    "g.set(xlabel='Month', ylabel='Number of events detected')\n",
    "g._legend.set_title('Event')\n",
    "g.savefig('BarPlotNumberEvents.png', dpi = 800, bbox_inches = 'tight') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate length and beam attributes\n",
    "data_raw = data_raw[data_raw['ID_EVENT_TYPE'].isin([6,7,27])]\n",
    "data_raw['length'] = data_raw['AIS_TO_BOW'] + data_raw['AIS_TO_STERN']\n",
    "data_raw['width'] = data_raw['AIS_TO_PORT'] + data_raw['AIS_TO_STARBOARD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We represent the characteristics calculated for each of the events through a scatter plot with distribution of the variables in the axes.\n",
    "g6 = sns.jointplot(\"length\", \"width\", data = data_raw[data_raw['ID_EVENT_TYPE'] == 6], kind = 'reg')\n",
    "g6.fig.suptitle(\"Pilotage Ship Specs\")\n",
    "g6.fig.tight_layout()\n",
    "g6.set_axis_labels(\"Length (m)\", \"Width (m)\")\n",
    "g6.fig.subplots_adjust(top=0.95)\n",
    "g6.savefig('PilotageShipSpecs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g7 = sns.jointplot(\"length\", \"width\", data = data_raw[data_raw['ID_EVENT_TYPE'] == 7], kind = 'reg')\n",
    "g7.fig.suptitle(\"Towage Ship Specs\")\n",
    "g7.fig.tight_layout()\n",
    "g7.set_axis_labels(\"Length (m)\", \"Width (m)\")\n",
    "g7.fig.subplots_adjust(top=0.95)\n",
    "g7.savefig(\"TowageShipSpecs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g27 = sns.jointplot(\"length\", \"width\", data = data_raw[data_raw['ID_EVENT_TYPE'] == 27], kind = 'reg')\n",
    "g27.fig.suptitle(\"Speeding Ship Specs\")\n",
    "g27.fig.tight_layout()\n",
    "g27.set_axis_labels(\"Length (m)\", \"Width (m)\")\n",
    "g27.fig.subplots_adjust(top=0.95)\n",
    "g27.savefig(\"SpeedingSpeds.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the following graph we represent the properties of the three events to see the differences between them.\n",
    "data_raw['event_desc'] = data_raw['ID_EVENT_TYPE'].map(dict_events)\n",
    "grid = sns.JointGrid(x = 'length', y = 'width', data = data_raw)\n",
    "g = grid.plot_joint(sns.scatterplot, hue = 'event_desc', data = data_raw)\n",
    "\n",
    "sns.kdeplot(data_raw[data_raw['event_desc'] == 'Pilotage Start']['length'], ax = g.ax_marg_x, legend = False)\n",
    "sns.kdeplot(data_raw[data_raw['event_desc'] == 'Towage Start']['length'], ax = g.ax_marg_x, legend = False)\n",
    "sns.kdeplot(data_raw[data_raw['event_desc'] == 'Speed Exceed']['length'], ax = g.ax_marg_x, legend = False)\n",
    "\n",
    "sns.kdeplot(data_raw[data_raw['event_desc'] == 'Pilotage Start']['width'], ax = g.ax_marg_y, legend = False)\n",
    "sns.kdeplot(data_raw[data_raw['event_desc'] == 'Towage Start']['width'], ax = g.ax_marg_y, legend = False)\n",
    "sns.kdeplot(data_raw[data_raw['event_desc'] == 'Speed Exceed']['width'], ax = g.ax_marg_y, legend = False)\n",
    "g.savefig(\"ScatterUB_Events.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMSI AND SHIP TYPE ARE CATEGORICAL VARIABLES\n",
    "data_clean['MMSI'] = data_clean['MMSI'].astype(str)\n",
    "data_clean['SHIP_TYPE'] = data_clean['SHIP_TYPE'].astype(str)\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We make a firts approach using Random Forest\n",
    "features = data_clean[['LON', 'LAT', 'LENGTH', 'WIDTH', 'SPEED', 'MMSI', 'SHIP_TYPE']]\n",
    "labels = data_clean['EVENT_DESC']\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, stratify = labels, test_size = 0.25)\n",
    "forest = RandomForestClassifier(n_estimators=25, criterion = 'gini', random_state= 1, n_jobs= -1)\n",
    "forest.fit(X_train, y_train)\n",
    "y_pred = forest.predict(X_test)\n",
    "\n",
    "print(\"Report: \", classification_report(y_test, y_pred))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F1_score: \", f1_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"precision_score: \", precision_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"recall: \", recall_score(y_test, y_pred, average = 'weighted'))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feat_labels = features.columns\n",
    "importances = forest.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" %(f+1, 30, feat_labels[indices[f]], importances[indices[f]])\n",
    "\n",
    "#We represent a bar plot with features importances given by Random Forest\n",
    "plt.title('Feature Importance')\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align = 'center')\n",
    "plt.xticks(range(X_train.shape[1]), feat_labels[indices], rotation = -45)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"FeaturesImportance.png\")\n",
    "plt.ylabel('Gini Importance Values')\n",
    "plt.xlabel('Features')\n",
    "plt.savefig('FeaturesImportance.png', dpi = 800, bbox_inches = 'tight' )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We repeat the process removing SHIP_TYPE and MMSI categories.\n",
    "features = data_clean[['LON', 'LAT', 'LENGTH', 'WIDTH', 'SPEED']]\n",
    "labels = data_clean['EVENT_DESC']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, stratify = labels, test_size = 0.25)\n",
    "forest = RandomForestClassifier(n_estimators=25, criterion = 'gini', random_state= 1, n_jobs= -1)\n",
    "forest.fit(X_train, y_train)\n",
    "y_pred = forest.predict(X_test)\n",
    "\n",
    "print(\"Report: \", classification_report(y_test, y_pred))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F1_score: \", f1_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"precision_score: \", precision_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"recall: \", recall_score(y_test, y_pred, average = 'weighted'))\n",
    "\n",
    "feat_labels = features.columns\n",
    "importances = forest.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" %(f+1, 30, feat_labels[indices[f]], importances[indices[f]]))\n",
    "\n",
    "plt.title('Feature Importance')\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align = 'center')\n",
    "plt.xticks(range(X_train.shape[1]), feat_labels[indices], rotation = -45)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use sequential feature selection is used in order to see which features make the most accuracy prediction\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from varios import SBS\n",
    "\n",
    "sbs = SBS(forest, k_features = 2)\n",
    "sbs.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot with scores of each combinatios computed\n",
    "k_feat = [len(k) for k in sbs.subsets_]\n",
    "plt.plot(k_feat, sbs.scores_, marker = 'o')\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of features')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"LinePlot_DroppingFeatures.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combination with most accuracy\n",
    "indices = list(sbs.subsets_[1])\n",
    "features.columns[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now Principal Component Analysis is performed to see which directions explain the most variance of the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "features_sc = sc.fit_transform(features)\n",
    "\n",
    "cov_mat = np.cov(features_sc.T)\n",
    "eigen_values, eigen_vectors = np.linalg.eig(cov_mat)\n",
    "tot = sum(eigen_values)\n",
    "var_exp = [(i / tot) for i in sorted(eigen_values, reverse = True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "plt.bar(range(1,6), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\n",
    "plt.step(range(1, 6), cum_var_exp, where= 'mid', label = 'Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc = 'best')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ExplainedVariance.png\", dpi = 800, bbox_inches = 'tight' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_pairs = [(np.abs(eigen_values[i]), eigen_vectors[:, i]) for i in range(len(eigen_values))]\n",
    "eigen_pairs.sort(key = lambda k: k[0], reverse = True)\n",
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis], eigen_pairs[1][1][:, np.newaxis], eigen_pairs[2][1][:, np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pca = features_sc.dot(w)\n",
    "features_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter plot of data points taken into this new dimensional space\n",
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "\n",
    "for l, c, m in zip (np.unique(labels), colors, markers):\n",
    "    plt.scatter(features_pca[labels == l][0], features_pca[labels == l][0], c=c, label = l, marker = m)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pca[labels == np.unique(labels)[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest is applied to see if PCA improved the accuracy\n",
    "features_pca = pd.DataFrame(features_pca)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_pca, labels, stratify = labels, test_size = 0.25)\n",
    "forest = RandomForestClassifier(n_estimators=25, criterion = 'gini', random_state= 1, n_jobs= -1)\n",
    "forest.fit(X_train, y_train)\n",
    "y_pred = forest.predict(X_test)\n",
    "\n",
    "print(\"Report: \", classification_report(y_test, y_pred))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F1_score: \", f1_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"precision_score: \", precision_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"recall: \", recall_score(y_test, y_pred, average = 'weighted'))\n",
    "\n",
    "\n",
    "feat_labels = features_pca.columns\n",
    "importances = forest.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" %(f+1, 30, feat_labels[indices[f]], importances[indices[f]]))\n",
    "\n",
    "plt.title('Feature Importance')\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align = 'center')\n",
    "plt.xticks(range(X_train.shape[1]), feat_labels[indices], rotation = -45)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.tight_layout()\n",
    "plt.savefig('Feature_ImportancesPCA.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A new feature extraction technique is used, where directions which maximizes class separability are the new objective\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "lda = LDA(n_components= 3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_sc, labels, stratify = labels, test_size = 0.25)\n",
    "X_train = lda.fit_transform(X_train, y_train)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=25, criterion = 'gini', random_state= 1, n_jobs= -1)\n",
    "forest.fit(X_train, y_train)\n",
    "X_test = lda.transform(X_test)\n",
    "y_pred = forest.predict(X_test)\n",
    "\n",
    "print(\"Report: \", classification_report(y_test, y_pred))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F1_score: \", f1_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"precision_score: \", precision_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"recall: \", recall_score(y_test, y_pred, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of PCA, we try Kernel Principal Component Analysis with Radial Basis Function Kernel, if there is some relation non linearly that was not seen.\n",
    "from sklearn.decomposition import KernelPCA\n",
    "scikit_kpca = KernelPCA(n_components = 2, kernel = 'rbf', gamma = 15 )\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_sc, labels, stratify = labels, test_size = 0.25)\n",
    "X_train = scikit_kpca.fit_transform(X_train, y_train)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=25, criterion = 'gini', random_state= 1, n_jobs= -1)\n",
    "forest.fit(X_train, y_train)\n",
    "X_test = scikit_kpca.transform(X_test)\n",
    "y_pred = forest.predict(X_test)\n",
    "\n",
    "print(\"Report: \", classification_report(y_test, y_pred))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F1_score: \", f1_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"precision_score: \", precision_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"recall: \", recall_score(y_test, y_pred, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We repeat the full process but using K-Nearest-Neighbots\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_sc, labels, stratify = labels, test_size = 0.25)\n",
    "knn = KNeighborsClassifier(n_neighbors = 5, p = 2, metric = 'minkowski')\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Report: \", classification_report(y_test, y_pred))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F1_score: \", f1_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"precision_score: \", precision_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"recall: \", recall_score(y_test, y_pred, average = 'weighted'))\n",
    "\n",
    "lda = LDA(n_components= 3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_sc, labels, stratify = labels, test_size = 0.25)\n",
    "X_train = lda.fit_transform(X_train, y_train)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5, p = 2, metric = 'minkowski')\n",
    "knn.fit(X_train, y_train)\n",
    "X_test = lda.transform(X_test)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Report: \", classification_report(y_test, y_pred))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F1_score: \", f1_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"precision_score: \", precision_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"recall: \", recall_score(y_test, y_pred, average = 'weighted'))\n",
    "\n",
    "scikit_kpca = KernelPCA(n_components = 2, kernel = 'rbf', gamma = 15 )\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_sc, labels, stratify = labels, test_size = 0.25)\n",
    "X_train = scikit_kpca.fit_transform(X_train, y_train)\n",
    "\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "X_test = scikit_kpca.transform(X_test)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Report: \", classification_report(y_test, y_pred))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F1_score: \", f1_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"precision_score: \", precision_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"recall: \", recall_score(y_test, y_pred, average = 'weighted'))\n",
    "\n",
    "pca = PCA(n_components= 2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_sc, labels, stratify = labels, test_size = 0.25)\n",
    "X_train = pca.fit_transform(X_train, y_train)\n",
    "\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "X_test = pca.transform(X_test)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Report: \", classification_report(y_test, y_pred))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F1_score: \", f1_score(y_test, y_pred, average = 'macro'))\n",
    "print(\"precision_score: \", precision_score(y_test, y_pred, average = 'macro'))\n",
    "print(\"recall: \", recall_score(y_test, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we perfom prediction of an event given the sequence of AIS Messages. \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "data_raw = pd.read_csv('C:/Users/sergi/Desktop/Pixel/CEP/Events_BBDD.csv')\n",
    "data_raw['AIS_UPDATE_DATE'] = pd.to_datetime(data_raw['AIS_UPDATE_DATE'], format = '%Y-%m-%d %H:%M:%S')\n",
    "data_raw = data_raw[data_raw['AIS_UPDATE_DATE'] > '2019-07-15']\n",
    "data = data_raw.copy(deep = True)\n",
    "data = data[data['ID_EVENT_TYPE'].isin([6,7,27])]\n",
    "data = data[(data['AIS_LAT'] < 90) & (data['AIS_LAT'] > -90) ]\n",
    "data = data[(data['AIS_LON'] < 180) & (data['AIS_LON'] > -180)]\n",
    "#data = data.dropna(subset = ['AIS_LAT', 'AIS_LON'])\n",
    "dict_events = {'others' : 'Others Events', 6 : 'Pilotage Start', 7: 'Towage Start', 27: 'Speed Exced'}\n",
    "data['ID_EVENT_TYPE'] = data['ID_EVENT_TYPE'].map(dict_events)\n",
    "#data.head()\n",
    "\n",
    "# All the AIS messages from the logs are retrieved\n",
    "df = pd.read_csv('C:/Users/sergi/Desktop/Pixel/CEP/LogsGrouped.csv')\n",
    "\n",
    "# AIS messages are filtered to only retrieve ones with valid locations\n",
    "df = df[(abs(df['lat']) < 90) & (abs(df['lon']) < 180)]\n",
    "df['updateAsDate'] = pd.to_datetime(df['updateAsDate'], format='%d/%m/%Y %H:%M:%S')\n",
    "df\n",
    "\n",
    "def feature_eng(events, df, rows):\n",
    "    labels_array = []\n",
    "    features_array = []\n",
    "    i = 0\n",
    "    for index, row in events.iterrows():\n",
    "\n",
    "        label = row['ID_EVENT_TYPE']\n",
    "        df_temp = pd.DataFrame()\n",
    "        mmsi = row['AIS_MMSI']\n",
    "        time = row['AIS_UPDATE_DATE']\n",
    "        df_temp = df[df['mmsi'] == mmsi]\n",
    "        df_temp = df_temp[df_temp['updateAsDate'] <= time]\n",
    "        df_temp.sort_values(by = ['updateAsDate'], inplace = True, ascending = False)\n",
    "        df_temp = df_temp.head(rows)\n",
    "        \n",
    "        if df_temp.shape[0]==30:\n",
    "            df_temp['length'] = df_temp['to_bow'] + df_temp['to_stern']\n",
    "            df_temp['width'] = df_temp['to_port'] + df_temp['to_starboard']\n",
    "            df_temp['Av.Speed'] = df_temp['speed'].expanding().mean()\n",
    "            df_temp = df_temp[['lat', 'lon', 'speed', 'length', 'width', 'course', 'turn', 'draught', 'heading' ]]\n",
    "            scaler = StandardScaler()\n",
    "            df_temp = scaler.fit_transform(df_temp)\n",
    "            features_array.append(df_temp.tolist())\n",
    "            labels_array.append(label)\n",
    "            i = i + 1\n",
    "\n",
    "    return features_array, labels_array, df_temp\n",
    "\n",
    "\n",
    "features_array, labels_arraty, prueba = feature_eng(data, df, 30)\n",
    "\n",
    "labels_array = pd.DataFrame(labels_arraty, columns = ['ID_EVENT'])\n",
    "labels_array = pd.get_dummies(labels_array, columns = ['ID_EVENT'])\n",
    "labels_array.values.tolist()\n",
    "\n",
    "features_array = np.asarray(features_array)\n",
    "labels_array = np.asarray(labels_array)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_array, labels_array, test_size = 0.25)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape = (30,9)))\n",
    "model.add(Dense(3, activation = 'sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.savefig(\"C:/Users/sergi/Desktop/Pixel/CEP/Loss.png\")\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.savefig(\"C:/Users/sergi/Desktop/Pixel/CEP/Accuracy.png\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
